# ==========================================
# 1. THE STRUCTURE (Neo4j Graph Database)
# ==========================================
# The URI uses the 'bolt' protocol, which is a binary protocol 
# optimized for performance (faster than standard HTTP).
spring.neo4j.uri=bolt://localhost:7687

# The credentials you defined in your docker-compose.yml
spring.neo4j.authentication.username=neo4j
spring.neo4j.authentication.password=vekku_secret

# ==========================================
# 2. THE MEMORY (Qdrant Vector Database)
# ==========================================
# Connects to the Qdrant container where your vectors (embeddings) live.
# Port 6333 is the HTTP/REST API port used by Spring AI.
spring.ai.vectorstore.qdrant.host=localhost
spring.ai.vectorstore.qdrant.port=6334
spring.ai.vectorstore.qdrant.initialize-schema=true 

# ==========================================
# 3. THE BRAIN (Local AI Model)
# ==========================================
# We are overriding the default model (MiniLM) with "BGE-Small v1.5".
# This model is smarter at understanding semantic meaning but still runs fast on CPU.

# The ONNX file: The actual neural network weights (the "Brain").
spring.ai.embedding.transformer.onnx.model-uri=https://huggingface.co/Xenova/bge-small-en-v1.5/resolve/main/onnx/model.onnx

# The Tokenizer: The logic that breaks words into numbers the brain can understand.
spring.ai.embedding.transformer.tokenizer.uri=https://huggingface.co/Xenova/bge-small-en-v1.5/resolve/main/tokenizer.json

# Ensures the model runs in "Embedding" mode (generating vectors) 
# rather than other NLP tasks.
spring.ai.embedding.transformer.metadata-mode=EMBED